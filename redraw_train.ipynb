{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pa9oTbZXovX",
        "outputId": "0d9b9b1c-566d-498b-d510-1cb7f8bd9f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  8 14:38:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0    35W /  70W |    374MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "    base = 'gdrive/MyDrive/vocr/'\n",
        "\n",
        "#Runtime Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrnKX9eIGzK2",
        "outputId": "a51ded56-af73-4770-a27b-9be74ec892c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!rm -rf Partitioned-Organic-Data-Split\n",
        "print(\"Unzipping...\")\n",
        "!unzip {base}Partitioned-Organic-Data-Split.zip>/dev/null\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATuG8sOViYQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from random import randint\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.applications import MobileNetV3Small, MobileNetV3Large, ResNet50V2\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
        "from tensorflow.keras.layers import Dense, add, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, Input, Concatenate, Cropping2D, ActivityRegularization, RandomBrightness, RandomContrast, RandomCrop, RandomFlip, RandomHeight, RandomRotation, RandomTranslation, RandomWidth, RandomZoom\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import codecs\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import runtime\n",
        "from scipy.io import loadmat\n",
        "import multiprocessing\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny6SVPtmYFHl"
      },
      "outputs": [],
      "source": [
        "# Util functions\n",
        "def log(message):\n",
        "\tprint(message)\n",
        "\tfile = codecs.open(base+\"log.txt\", \"a\", \"cp1252\", \"replace\")\n",
        "\tprint(message, file=file)\n",
        "\n",
        "def load_img(path, label):\n",
        "\timg = tf.io.read_file(path)\n",
        "\timg = tf.io.decode_png(img, channels=3)\n",
        "\timg = tf.cast(img, tf.float16)\n",
        "\timg = tf.image.resize(img, (224,224))\n",
        "\treturn img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxjjkWeKYqsQ"
      },
      "outputs": [],
      "source": [
        "class MetricCallback(Callback):\n",
        "\tdef __init__(self, x, y, k=0, patience=0, restore_best=True):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\tself.k=k\n",
        "\t\tself.best_score = 0.0\n",
        "\t\tself.best_epoch = 0\n",
        "\t\tself.best_weights = None\n",
        "\t\tself.patience = patience\n",
        "\t\tself.restore_best = restore_best\n",
        "\n",
        "\tdef restore(self):\n",
        "\t\tself.model.set_weights(self.best_weights)\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\tpreds = self.model.predict(self.x)\n",
        "\t\tfinal = f1_score(self.y.argmax(-1), preds.argmax(-1), average='micro')\n",
        "\t\tif final>self.best_score:\n",
        "\t\t\tfor f in glob(\"*.h5\"): os.remove(f)\n",
        "\t\t\tself.model.save(f\"model {self.k+1} {epoch+1} f{final:.3f}.h5\")\n",
        "\t\t\t!rm {base}*.h5\n",
        "\t\t\t!cp *.h5 {base}\n",
        "\t\t\tlog(f\"{epoch+1} Best Score: improved from {self.best_score:.3f} to {final:.3f}\")\n",
        "\t\t\tself.best_score = final\n",
        "\t\t\tself.best_epoch = epoch\n",
        "\t\t\tself.best_weights = self.model.get_weights()\n",
        "\t\telse: log(f\"{epoch+1} Score: {final:.3f}, Best epoch: {self.best_epoch+1}, {self.best_score:.3f}\")\n",
        "\n",
        "\t\tif self.patience>0 and epoch-self.best_epoch >= self.patience:\n",
        "\t\t\tprint(\"Stopping...\")\n",
        "\t\t\tself.stopped_epoch = epoch\n",
        "\t\t\tself.model.stop_training = True\n",
        "\t\t\tif self.restore_best: self.restore()\n",
        "\t\t\treturn\n",
        "\n",
        "\t\tif not epoch == self.best_epoch and (epoch-self.best_epoch)%int(self.patience/2) == 0:\n",
        "\t\t\told_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "\t\t\tnew_lr = old_lr * 0.1\n",
        "\t\t\ttf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "\t\t\tself.restore()\n",
        "\t\t\tlog(f\"Reducing learning rate from {old_lr} to {new_lr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7Qz6pFhZDGo"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "def describe(model):\n",
        "\tfor i, layer in enumerate(model.layers):\n",
        "\t\tif layer.trainable: print(i, layer.name, layer.trainable, layer.input_shape, layer.output_shape)\n",
        "\t\tif hasattr(layer, \"layers\"): describe(layer)\n",
        "\n",
        "def create_model(input_shape, output_shape):\n",
        "\tdata_augmentation = Sequential([\n",
        "\t\tRandomBrightness(0.2),\n",
        "\t\tRandomContrast(0.2),\n",
        "\t\tRandomFlip(),\n",
        "\t\tRandomRotation(0.2),\n",
        "\t\tRandomTranslation(0.2, 0.2),\n",
        "\t\tRandomZoom(0.2, 0.2)\n",
        "\t])\n",
        "\tbase_model = MobileNetV3Small(include_top=False, input_shape=input_shape, weights='imagenet', pooling=\"avg\")\n",
        "\tbase_model.trainable = False\n",
        "\tinputs = Input(shape=input_shape)\n",
        "\tx = data_augmentation(inputs)\n",
        "\tx = base_model(x, training=False)\n",
        "\tx = Dense(256)(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = Activation(\"relu\")(x)\n",
        "\tx = Dropout(0.2)(x)\n",
        "\toutputs =Dense(output_shape,activation='softmax')(x)\n",
        "\tmodel=Model(inputs=inputs, outputs=outputs)\n",
        "\tmodel.compile(optimizer=Adam(1e-3), loss='CategoricalCrossentropy',metrics=['accuracy'])\n",
        "\tdescribe(model)\n",
        "\tprint(model.summary())\n",
        "\treturn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaQJs_5pG-Fi"
      },
      "outputs": [],
      "source": [
        "#Imblearn\n",
        "def balance(x, y):\n",
        "\tprint(\"Balancing samples...\")\n",
        "\tros = RandomOverSampler()\n",
        "\tx, y = ros.fit_resample(x.reshape(-1,1), y)\n",
        "\t# Can't use synthetic for memory limitation\n",
        "\t#smote_enn = SMOTEENN(random_state=0)\n",
        "\t#x, y = smote_enn.fit_resample(x, y)\n",
        "\tx = x.reshape(-1)\n",
        "\tu, c = np.unique(y, return_counts=True)\n",
        "\tnames = [le.classes_[l] for l in u]\n",
        "\tdist = list(zip(names, c))\n",
        "\tdist = sorted(dist, key = lambda x: x[1])\n",
        "\tprint(\"Distribution after over sampling:\", dist)\n",
        "\treturn x, y\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8d8jD0GPspR",
        "outputId": "20ee8cbb-cd77-4b76-8f0a-5988bf530a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting at: 12/08/2022, 14:40:23\n",
            "Size: (224, 224, 3), batch_size: 32, k_fold: 0\n",
            "Tensorflow: 2.9.2\n",
            "SKLearn: 1.0.2\n"
          ]
        }
      ],
      "source": [
        "size = (224, 224, 3)\n",
        "batch_size = 32\n",
        "k=0\n",
        "t_start = time.time()\n",
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "log(f\"Starting at: {date_time}\")\n",
        "log(f\"Size: {size}, batch_size: {batch_size}, k_fold: {k}\")\n",
        "log(f\"Tensorflow: {tf.version.VERSION}\")\n",
        "log(f\"SKLearn: {sklearn.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ex6JzOpffOH",
        "outputId": "89f43fe2-6abf-4d81-999b-730900715210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TextView', 'ImageView', 'Button', 'Switch', 'EditText', 'ImageButton', 'CheckedTextView', 'ProgressBarHorizontal', 'RatingBar', 'ProgressBarVertical', 'ToggleButton', 'CheckBox', 'Spinner', 'SeekBar', 'NumberPicker', 'RadioButton']\n",
            "True True\n"
          ]
        }
      ],
      "source": [
        "train = loadmat(\"Partitioned-Organic-Data-Split/organic-training.mat\")\n",
        "x_train = train['trainingImages'].flatten()\n",
        "x_train = [x[0][x[0].index(\"Partitioned\"):] for x in x_train]\n",
        "x_train = np.array(x_train)\n",
        "y_train = train['labels'].flatten().astype(int)\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weight = dict(zip(np.unique(y_train), weight))\n",
        "y_train= to_categorical(y_train)\n",
        "\n",
        "validation = loadmat(\"Partitioned-Organic-Data-Split/organic-validation.mat\")\n",
        "x_validation = validation['trainingImages'].flatten()\n",
        "x_validation = [x[0][x[0].index(\"Partitioned\"):] for x in x_validation]\n",
        "x_validation = np.array(x_validation)\n",
        "y_validation = validation['labels'].flatten().astype(int)\n",
        "y_validation = le.transform(y_validation)\n",
        "y_validation = to_categorical(y_validation)\n",
        "\n",
        "test = loadmat(\"Partitioned-Organic-Data-Split/organic-test.mat\")\n",
        "x_test = test['trainingImages'].flatten()\n",
        "x_test = [x[0][x[0].index(\"Partitioned\"):] for x in x_test]\n",
        "x_test = np.array(x_test)\n",
        "y_test = test['labels'].flatten().astype(int)\n",
        "y_test = le.transform(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "log(f\"x_train: {x_train.shape}, y_train:{y_train.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OvfFhU5Ie8cB",
        "outputId": "e357d28a-eb7b-4b8e-d3c4-723ab33fea16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 0...\n",
            "0 input_38 True [(None, 224, 224, 3)] [(None, 224, 224, 3)]\n",
            "1 sequential_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "0 random_brightness_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "1 random_contrast_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "2 random_flip_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "3 random_rotation_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "4 random_translation_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "5 random_zoom_18 True (None, 224, 224, 3) (None, 224, 224, 3)\n",
            "3 dense_36 True (None, 576) (None, 256)\n",
            "4 batch_normalization_18 True (None, 256) (None, 256)\n",
            "5 activation_18 True (None, 256) (None, 256)\n",
            "6 dropout_18 True (None, 256) (None, 256)\n",
            "7 dense_37 True (None, 256) (None, 16)\n",
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_38 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " sequential_18 (Sequential)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " MobilenetV3small (Functiona  (None, 576)              939120    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 256)               147712    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 16)                4112      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,091,968\n",
            "Trainable params: 152,336\n",
            "Non-trainable params: 939,632\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/1000\n",
            " 272/4473 [>.............................] - ETA: 29:44 - loss: 1.1033 - accuracy: 0.6488"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-39e4278150e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#weight = dict(zip(np.unique(y_train), weight))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, class_weight=weight) #, workers=multiprocessing.cpu_count(), use_multiprocessing=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Train\n",
        "log(f\"Training fold {k}...\")\n",
        "model = create_model(size, y_train.shape[1])\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds= train_ds.shuffle(x_train.shape[0]).map(load_img, num_parallel_calls=batch_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((x_validation, y_validation))\n",
        "validation_ds= validation_ds.map(load_img, num_parallel_calls=batch_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "mc = MetricCallback(validation_ds, y_validation, patience=50)\n",
        "logger = CSVLogger(base+'train.csv')\n",
        "callbacks = [mc, logger]\n",
        "history = model.fit(train_ds, validation_data=validation_ds, epochs=1000, callbacks =callbacks, workers=multiprocessing.cpu_count(), use_multiprocessing=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrYuVHAzfBSe"
      },
      "outputs": [],
      "source": [
        "#Predict\n",
        "mc.restore()\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds= test_ds.map(load_img, num_parallel_calls=batch_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "preds = model.predict(test_ds)\n",
        "score = precision_recall_fscore_support(y_test.argmax(-1), preds.argmax(-1), average='micro')\n",
        "print(f\"Micro Precision: {score[0]:.2f}, Recall: {score[1]:.2f}, F1: {score[2]:.2f}\")\n",
        "score = precision_recall_fscore_support(y_test.argmax(-1), preds.argmax(-1), average='weighted')\n",
        "print(f\"Weighted Precision: {score[0]:.2f}, Recall: {score[1]:.2f}, F1: {score[2]:.2f}\")\n",
        "score = precision_recall_fscore_support(y_test.argmax(-1), preds.argmax(-1), average='macro')\n",
        "print(f\"Macro Precision: {score[0]:.2f}, Recall: {score[1]:.2f}, F1: {score[2]:.2f}\")\n",
        "matrix = confusion_matrix(y_test.argmax(-1), preds.argmax(-1))\n",
        "print(\"Confusion Matrix:\")\n",
        "for i,m in enumerate(matrix):\n",
        "  row = [(le.classes_[j], k) for j, k in enumerate(m) if k>0]\n",
        "  print(i, le.classes_[i], row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#End\n",
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "log(f\"Ending at: {date_time}\")\n",
        "t_finish = time.time()\n",
        "total_time = (t_finish-t_start)/60\n",
        "log(f\"It took {total_time:.2f} minutes\")\n",
        "time.sleep(1)\n",
        "#runtime.unassign()"
      ],
      "metadata": {
        "id": "0qnKz5UYol16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtm7OWhh/Mkq9xPsHTrBL5"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}